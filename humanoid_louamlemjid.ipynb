{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/louamlemjid/project06-humanoid-obj-manipulation/blob/master/humanoid_louamlemjid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Supprimer le dossier repo dans colab !!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm -r project06-humanoid-obj-manipulation #7ot repo mte3ik win XML files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "importer le repo :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGIQ8MNuRLZm",
        "outputId": "988a6b7d-96ae-4709-9afd-3b8e38f2dd50"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/louamlemjid/project06-humanoid-obj-manipulation.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZbvdYvhRzf4"
      },
      "source": [
        "load model\n",
        "**H1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51dabd8d"
      },
      "source": [
        "# Task\n",
        "Install dm_control and its dependencies, then load the \"h1.xml\" MuJoCo model, create a physics environment, render the simulation, and display the rendering in the Colab notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b03aa7a"
      },
      "source": [
        "## Install dm control\n",
        "\n",
        "### Subtask:\n",
        "Install the `dm_control` library and its dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbZxYDxzoz5R",
        "outputId": "7e93bf5c-a5a6-43dc-ca9c-b698b0cfa393"
      },
      "outputs": [],
      "source": [
        "#@title Run to install MuJoCo and `dm_control`\n",
        "import distutils.util\n",
        "import os\n",
        "import subprocess\n",
        "if subprocess.run('nvidia-smi').returncode:\n",
        "  raise RuntimeError(\n",
        "      'Cannot communicate with GPU. '\n",
        "      'Make sure you are using a GPU Colab runtime. '\n",
        "      'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
        "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
        "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
        "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
        "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "    f.write(\"\"\"{\n",
        "    \"file_format_version\" : \"1.0.0\",\n",
        "    \"ICD\" : {\n",
        "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "print('Installing dm_control...')\n",
        "!pip install -q dm_control>=1.0.31\n",
        "\n",
        "# Configure dm_control to use the EGL rendering backend (requires GPU)\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "print('Checking that the dm_control installation succeeded...')\n",
        "try:\n",
        "  from dm_control import suite\n",
        "  env = suite.load('cartpole', 'swingup')\n",
        "  pixels = env.physics.render()\n",
        "except Exception as e:\n",
        "  raise e from RuntimeError(\n",
        "      'Something went wrong during installation. Check the shell output above '\n",
        "      'for more information.\\n'\n",
        "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "else:\n",
        "  del pixels, suite\n",
        "\n",
        "!echo Installed dm_control $(pip show dm_control | grep -Po \"(?<=Version: ).+\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1d369"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the `dm_control` library and its dependencies using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T5f4w3Kq2X14"
      },
      "outputs": [],
      "source": [
        "#@title All `dm_control` imports required for this tutorial\n",
        "\n",
        "# The basic mujoco wrapper.\n",
        "from dm_control import mujoco\n",
        "\n",
        "# Access to enums and MuJoCo library functions.\n",
        "from dm_control.mujoco.wrapper.mjbindings import enums\n",
        "from dm_control.mujoco.wrapper.mjbindings import mjlib\n",
        "\n",
        "# Composer high level imports\n",
        "from dm_control import composer\n",
        "from dm_control.composer.observation import observable\n",
        "from dm_control.composer import variation\n",
        "\n",
        "# Imports for Composer tutorial example\n",
        "from dm_control.composer.variation import distributions\n",
        "from dm_control.composer.variation import noises\n",
        "from dm_control.locomotion.arenas import floors\n",
        "\n",
        "# Control Suite\n",
        "from dm_control import suite\n",
        "\n",
        "# Run through corridor example\n",
        "from dm_control.locomotion.walkers import cmu_humanoid\n",
        "from dm_control.locomotion.arenas import corridors as corridor_arenas\n",
        "from dm_control.locomotion.tasks import corridors as corridor_tasks\n",
        "\n",
        "# Soccer\n",
        "from dm_control.locomotion import soccer\n",
        "\n",
        "# Manipulation\n",
        "from dm_control import manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5c74ace"
      },
      "source": [
        "## Import necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Import modules from `dm_control` and other required libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKc1FNhKiVJX",
        "outputId": "a9cf1026-df05-43c4-a71e-f6e9be8d4f12"
      },
      "outputs": [],
      "source": [
        "#@title Other imports and helper functions\n",
        "\n",
        "# General\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "import itertools\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "\n",
        "# Graphics-related\n",
        "import matplotlib\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "import PIL.Image\n",
        "# Internal loading of video libraries.\n",
        "\n",
        "# Use svg backend for figure rendering\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "\n",
        "# Font sizes\n",
        "SMALL_SIZE = 8\n",
        "MEDIUM_SIZE = 10\n",
        "BIGGER_SIZE = 12\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
        "\n",
        "# Inline video helper function\n",
        "if os.environ.get('COLAB_NOTEBOOK_TEST', False):\n",
        "  # We skip video generation during tests, as it is quite expensive.\n",
        "  display_video = lambda *args, **kwargs: None\n",
        "else:\n",
        "  def display_video(frames, framerate=30):\n",
        "    height, width, _ = frames[0].shape\n",
        "    dpi = 70\n",
        "# Access to enums and MuJoCo library functions.\n",
        "    orig_backend = matplotlib.get_backend()\n",
        "    matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
        "    matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
        "    ax.set_axis_off()\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_position([0, 0, 1, 1])\n",
        "    im = ax.imshow(frames[0])\n",
        "    def update(frame):\n",
        "      im.set_data(frame)\n",
        "      return [im]\n",
        "    interval = 1000/framerate\n",
        "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                   interval=interval, blit=True, repeat=False)\n",
        "    return HTML(anim.to_html5_video())\n",
        "\n",
        "# Seed numpy's global RNG so that cell outputs are deterministic. We also try to\n",
        "# use RandomState instances that are local to a single cell wherever possible.\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73c41072"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary modules from dm_control and matplotlib as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "501357a3"
      },
      "source": [
        "## Load the mujoco model\n",
        "\n",
        "### Subtask:\n",
        "Load your `h1.xml` MuJoCo model using `dm_control`'s loading functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "742608f3"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the model path and load the MuJoCo model using `dm_control`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "8o57j34KfLVM",
        "outputId": "c7b19329-8f80-49fb-d8e5-742d9dac7a8c"
      },
      "outputs": [],
      "source": [
        "scene_option = mujoco.wrapper.core.MjvOption()\n",
        "scene_option.flags[enums.mjtVisFlag.mjVIS_JOINT] = True\n",
        "pixels = physics.render(scene_option=scene_option)\n",
        "PIL.Image.fromarray(pixels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPwP7wVtmnMe"
      },
      "source": [
        "### **DEBUT DE L'IMPLEMENTATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F0NQYlcz-efy"
      },
      "outputs": [],
      "source": [
        "# src/sim/scene_builder.py\n",
        "\n",
        "from dm_control import mujoco\n",
        "import os\n",
        "import PIL.Image\n",
        "class SceneBuilder:\n",
        "    \"\"\"\n",
        "    Responsible for building a complete MuJoCo scene directly\n",
        "    from a given XML file.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def build(scene_xml_path: str) -> mujoco.Physics:\n",
        "        \"\"\"\n",
        "        Loads and compiles a MuJoCo scene from a specified XML file.\n",
        "\n",
        "        This is a static method, so you can call it directly on the class\n",
        "        without creating an instance: `SceneBuilder.build_from_xml(...)`.\n",
        "\n",
        "        Args:\n",
        "            scene_xml_path: The full path to the scene's .xml file.\n",
        "\n",
        "        Returns:\n",
        "            A compiled dm_control.mujoco.Physics instance.\n",
        "\n",
        "        Raises:\n",
        "            FileNotFoundError: If the provided XML file path does not exist.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(scene_xml_path):\n",
        "            raise FileNotFoundError(\n",
        "                f\"The specified scene file was not found: {scene_xml_path}\"\n",
        "            )\n",
        "\n",
        "        print(f\"Loading scene from: {scene_xml_path}\")\n",
        "\n",
        "        # The core function to load a model from a complete XML file.\n",
        "        physics = mujoco.Physics.from_xml_path(scene_xml_path)\n",
        "\n",
        "        print(\"Scene built successfully.\")\n",
        "        return physics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGY2YS8_EoND"
      },
      "source": [
        "# instantiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TRhSU0wEnow",
        "outputId": "f7ac6142-6129-495c-ab24-fd2d78b5137d"
      },
      "outputs": [],
      "source": [
        "pathToModel = \"./project06-humanoid-obj-manipulation/models/dex_hand/cube_scene.xml\"\n",
        "sceneBuilder = SceneBuilder()\n",
        "physics = sceneBuilder.build(pathToModel)\n",
        "physics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atkzarCsFBZ8"
      },
      "source": [
        "# visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "4bA-bvlFFEkP",
        "outputId": "04ff562e-8df9-4bed-c2bf-792c386503cf"
      },
      "outputs": [],
      "source": [
        "scene_option = mujoco.wrapper.core.MjvOption()\n",
        "scene_option.flags[enums.mjtVisFlag.mjVIS_JOINT] = True\n",
        "pixels = physics.render(scene_option=scene_option)\n",
        "PIL.Image.fromarray(pixels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1FSDusRELY_"
      },
      "source": [
        "## dex hand class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfE7y4fSorLl",
        "outputId": "e8ca98c0-3f42-416d-c61c-7ba60d6283e2"
      },
      "outputs": [],
      "source": [
        "physics.named.data.xpos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf2FSGHKu9WW",
        "outputId": "d9b9fece-73f1-4db2-d7c5-9791fc0caea1"
      },
      "outputs": [],
      "source": [
        "physics.named.data.qpos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L-NNvBzw8kE"
      },
      "source": [
        "# Difference entre QPOS et XPOS ?????"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVP7FRfe3aLB",
        "outputId": "2d69f561-5745-4cb3-f577-ce0838c6fc86"
      },
      "outputs": [],
      "source": [
        "physics.model.actuator_ctrlrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "meSJxdxyCnnA"
      },
      "outputs": [],
      "source": [
        "# src/robot/base_robot.py\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "from dm_control import mujoco\n",
        "\n",
        "class BaseRobot(ABC):\n",
        "    \"\"\"Abstract base class for any robot in the simulation.\"\"\"\n",
        "\n",
        "    def __init__(self, physics: mujoco.Physics):\n",
        "        self._physics = physics\n",
        "        self._model = physics.model\n",
        "        self._controllable_joint_ids = []\n",
        "        self._actuator_ids = []\n",
        "\n",
        "    @property\n",
        "    def physics(self) -> mujoco.Physics:\n",
        "        return self._physics\n",
        "\n",
        "    @property\n",
        "    def model(self) -> mujoco.wrapper.core.MjModel:\n",
        "        return self._model\n",
        "\n",
        "    @property\n",
        "    def controllable_joint_ids(self) -> list[int]:\n",
        "        \"\"\"Returns a list of MuJoCo IDs for joints controlled by this robot.\"\"\"\n",
        "        return self._controllable_joint_ids\n",
        "\n",
        "    @property\n",
        "    def actuator_ids(self) -> list[int]:\n",
        "        \"\"\"Returns a list of MuJoCo IDs for actuators controlling this robot.\"\"\"\n",
        "        return self._actuator_ids\n",
        "\n",
        "    @property\n",
        "    def action_space_limits(self) -> tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Returns (low, high) bounds for the robot's action space.\"\"\"\n",
        "        low = self._model.actuator_ctrlrange[self.actuator_ids, 0]\n",
        "        high = self._model.actuator_ctrlrange[self.actuator_ids, 1]\n",
        "        return low, high\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_observation_data(self) -> np.ndarray:\n",
        "        \"\"\"Returns the part of the observation related to this robot's state.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def set_commands(self, actions: np.ndarray):\n",
        "        \"\"\"Applies actions to the robot's actuators.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset_state(self):\n",
        "        \"\"\"Resets the robot's joint positions and velocities to initial state.\"\"\"\n",
        "        pass\n",
        "class DexHandRobot(BaseRobot):\n",
        "    \"\"\"Concrete implementation for the Dex Hand robot.\"\"\"\n",
        "\n",
        "    def __init__(self, physics: mujoco.Physics):\n",
        "        super().__init__(physics)\n",
        "        # Identify specific joint and actuator IDs for the Dex Hand\n",
        "        # Filter for names starting with 'lh_' (left hand) and exclude wrist for finger-only control\n",
        "        self._controllable_joint_ids = [5,4,3,2,18,17,16,15,14,9,8,\n",
        "            7,6,13,12,11,10,23,22,21,20,19]\n",
        "        self._actuator_ids = [9, 8, 7, 19, 18, 17, 16, 12, 11, 10, 15, 14, 13, 6, 5, 4, 3, 2]\n",
        "\n",
        "\n",
        "        # Store initial qpos for the Dex Hand for reset (only for controllable joints)\n",
        "        self._initial_qpos = self.physics.data.qpos[self.controllable_joint_ids].copy()\n",
        "\n",
        "    def get_observation_data(self) -> np.ndarray:\n",
        "        \"\"\"Returns Dex Hand's qpos and qvel for controllable joints.\"\"\"\n",
        "        qpos_obs = self.physics.data.qpos[self.controllable_joint_ids]\n",
        "        qvel_obs = self.physics.data.qvel[self.controllable_joint_ids]\n",
        "        return np.concatenate([qpos_obs, qvel_obs]).astype(np.float32)\n",
        "\n",
        "    def set_commands(self, actions: np.ndarray):\n",
        "        \"\"\"Applies actions (target joint positions) to Dex Hand actuators.\"\"\"\n",
        "        # Actions array should directly correspond to the order of actuator_ids\n",
        "        for i, act_id in enumerate(self.actuator_ids):\n",
        "            self.physics.data.ctrl[act_id] = actions[i]\n",
        "\n",
        "    def reset_state(self):\n",
        "        \"\"\"Resets the Dex Hand's joint positions and velocities.\"\"\"\n",
        "        # Set qpos for controllable joints to their initial state\n",
        "        self.physics.data.qpos[self.controllable_joint_ids] = self._initial_qpos\n",
        "        # Set qvel for controllable joints to zero\n",
        "        self.physics.data.qvel[self.controllable_joint_ids] = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUzhzJumFMVJ"
      },
      "source": [
        "# instantiation dex hand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3rTQRkGL_JK",
        "outputId": "ce2ec947-e9b5-42ac-f7de-1f66f1569548"
      },
      "outputs": [],
      "source": [
        "robot = DexHandRobot(physics)\n",
        "robot.get_observation_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrYPHFBgOOYq"
      },
      "source": [
        "## object class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FjZ3rDlxORH6"
      },
      "outputs": [],
      "source": [
        "# src/objects/base_object.py\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "from dm_control import mujoco\n",
        "\n",
        "class BaseObject(ABC):\n",
        "    \"\"\"Abstract base class for any manipulable object.\"\"\"\n",
        "\n",
        "    def __init__(self, physics: mujoco.Physics, object_config: dict):\n",
        "        self._physics = physics\n",
        "        self._model = physics.model\n",
        "        self._object_config = object_config\n",
        "        self._body_id = self._model.name2id('cube','body')\n",
        "        self._joint_id = self._model.name2id('cube_joint','joint')\n",
        "\n",
        "    @property\n",
        "    def physics(self) -> mujoco.Physics:\n",
        "        return self._physics\n",
        "\n",
        "    @property\n",
        "    def model(self) -> mujoco.wrapper.core.MjModel:\n",
        "        return self._model\n",
        "\n",
        "    @property\n",
        "    def body_id(self) -> int:\n",
        "        return self._body_id\n",
        "\n",
        "    @property\n",
        "    def joint_id(self) -> int:\n",
        "        return self._joint_id\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_observation_data(self) -> np.ndarray:\n",
        "        \"\"\"Returns the part of the observation related to this object's state.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset_state(self):\n",
        "        \"\"\"Resets the object's position and orientation to an initial (possibly randomized) state.\"\"\"\n",
        "        pass\n",
        "\n",
        "# src/objects/cube_object.py\n",
        "import numpy as np\n",
        "from dm_control import mujoco\n",
        "class CubeObject(BaseObject):\n",
        "    \"\"\"Concrete implementation for a manipulable cube object.\"\"\"\n",
        "\n",
        "    def __init__(self, physics: mujoco.Physics, object_config: dict):\n",
        "        super().__init__(physics, object_config)\n",
        "        self._initial_pos_relative = np.array(object_config[\"initial_pos_relative\"])\n",
        "        self._pos_randomization_range = {\n",
        "            k: np.array(v) for k, v in object_config[\"pos_randomization_range\"].items()\n",
        "        }\n",
        "\n",
        "    def get_observation_data(self) -> np.ndarray:\n",
        "        \"\"\"Returns cube's position, orientation, linear and angular velocities.\"\"\"\n",
        "        object_pos = self._physics.data.xpos[self._body_id]\n",
        "        object_quat = self._physics.data.xquat[self._body_id]\n",
        "        object_lin_vel = self._physics.data.object_velocity(self.body_id,'body')[0]\n",
        "        object_ang_vel = self._physics.data.object_velocity(self.body_id,'body')[1]\n",
        "        return np.concatenate([object_pos, object_quat, object_lin_vel, object_ang_vel]).astype(np.float32)\n",
        "\n",
        "    def reset_state(self):\n",
        "        \"\"\"Resets the cube's position to initial (with randomization) and velocity to zero.\"\"\"\n",
        "        object_initial_qpos_idx = self._model.jnt_qposadr[self.joint_id]\n",
        "\n",
        "        # Apply randomization to initial position\n",
        "        random_offset_x = np.random.uniform(self._pos_randomization_range['x'][0], self._pos_randomization_range['x'][1])\n",
        "        random_offset_y = np.random.uniform(self._pos_randomization_range['y'][0], self._pos_randomization_range['y'][1])\n",
        "        random_offset_z = np.random.uniform(self._pos_randomization_range['z'][0], self._pos_randomization_range['z'][1])\n",
        "\n",
        "        new_pos = self._initial_pos_relative + np.array([random_offset_x, random_offset_y, random_offset_z])\n",
        "\n",
        "        # Set the object's position (first 3 values of its free joint qpos)\n",
        "        self._physics.data.qpos[object_initial_qpos_idx:object_initial_qpos_idx+3] = new_pos\n",
        "        # Set the object's orientation (quaternion, next 4 values). Keep it upright.\n",
        "        self._physics.data.qpos[object_initial_qpos_idx+3:object_initial_qpos_idx+7] = np.array([1.0, 0.0, 0.0, 0.0]) # WXYZ identity quaternion\n",
        "\n",
        "        # Reset object's velocities (6 values for a free joint)\n",
        "        object_initial_qvel_idx = self._model.jnt_dofadr[self.joint_id]\n",
        "        self._physics.data.qvel[object_initial_qvel_idx:object_initial_qvel_idx+6] = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EllqinMOuND"
      },
      "source": [
        "## Instantiation Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uxgbRLIDO1JF"
      },
      "outputs": [],
      "source": [
        "object_config = {\n",
        "    \"initial_pos_relative\": [0.0, 0.0, 0.0],\n",
        "    \"pos_randomization_range\": {\n",
        "        \"x\": [-0.1, 0.1],\n",
        "        \"y\": [-0.02, 0.02],\n",
        "    \"z\": [-0.005, 0.005]},\n",
        "    \"name\": \"cube\" ,\n",
        "  \"joint_name\": \"cube_joint\" ,\n",
        "  \"geom_name\": \"cube_geom\",\n",
        "  \"size\": [0.03, 0.04, 0.02],\n",
        "    \"rewards\":{\n",
        "            \"object_height_factor\": 100,\n",
        "                \"target_height\": 0.06,\n",
        "                \"target_height_bonus\": 500,\n",
        "                \"drop_penalty\": 200\n",
        "            },\n",
        "       \"env\":{\n",
        "           \"hand_model_path\": \"models/dex_hand/scene.xml\",\n",
        "           \"control_timestep\": 0.002,\n",
        "           \"episode_duration\": 2.0 # seconds\n",
        "       },\n",
        "    \"rl\":\n",
        "    {\n",
        "      \"total_timesteps\": 50000 ,# Keep low for initial test, increase to millions for actual training\n",
        "      \"log_dir\": \"./sac_hand_manipulation_logs/\",\n",
        "      \"model_save_path\": \"./trained_models/hand_policy.zip\",\n",
        "      \"learning_rate\": 0.0003,\n",
        "      \"buffer_size\": 10000,\n",
        "      \"learning_starts\": 100, # Start learning quickly for test\n",
        "      \"train_freq\": [1, \"episode\"],\n",
        "      \"gradient_steps\": 1,\n",
        "      \"ent_coef\": \"auto\",\n",
        "      \"save_freq\": 5000, # How often to save checkpoints during training}\n",
        "    }}\n",
        "\n",
        "manipulable_object = CubeObject(physics, object_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GMKTGI6scyF",
        "outputId": "3bd2ff2c-73ee-4795-facb-f4020c27da84"
      },
      "outputs": [],
      "source": [
        "manipulable_object.get_observation_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQbQ39_gw1zK"
      },
      "source": [
        "### observation builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OXUehAvdw6X7"
      },
      "outputs": [],
      "source": [
        "# src/env/observation_builder.py\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ObservationBuilder:\n",
        "    \"\"\"\n",
        "    Responsible for building the complete observation array from various components.\n",
        "    \"\"\"\n",
        "    def __init__(self, robot: BaseRobot, manipulable_object: BaseObject):\n",
        "        self._robot = robot\n",
        "        self._object = manipulable_object\n",
        "\n",
        "    def build_observation(self) -> np.ndarray:\n",
        "        \"\"\"Collects observation data from robot and object and concatenates them.\"\"\"\n",
        "        robot_obs = self._robot.get_observation_data()\n",
        "        object_obs = self._object.get_observation_data()\n",
        "\n",
        "        obs = np.concatenate([\n",
        "            robot_obs,\n",
        "            object_obs,\n",
        "        ]).astype(np.float32)\n",
        "        return obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY3mGMmhxC2R"
      },
      "source": [
        "## Instantiaiton observation builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJNz0swOxH5E",
        "outputId": "3cca1ab9-ccc3-4671-9841-fd74ace0a18e"
      },
      "outputs": [],
      "source": [
        " observation_builder = ObservationBuilder(robot, manipulable_object)\n",
        " observation_builder.build_observation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxcABkTrxoSV"
      },
      "source": [
        "## reward component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKVaKgiAsuVk",
        "outputId": "12213977-a7c0-4b5d-e4b8-cd79c6bc705f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.38657388,  1.44734049, -0.07510776])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "physics.data.xpos[manipulable_object.body_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jogxty4HxrqN"
      },
      "outputs": [],
      "source": [
        "# src/env/reward_components.py\n",
        "import numpy as np\n",
        "from dm_control import mujoco\n",
        "\n",
        "# Define individual reward functions here.\n",
        "# Each function takes physics, relevant object_body_id, and config.\n",
        "\n",
        "def object_height_reward(physics: mujoco.Physics, object_body_id: int, config: dict) -> float:\n",
        "    \"\"\"Rewards the agent based on the object's height.\"\"\"\n",
        "    object_height = physics.data.xpos[object_body_id][2] # Z-coordinate\n",
        "    reward = config.get(\"object_height_factor\", 100) * (object_height - 0.05) # Assume table is at Z=0.05\n",
        "    return float(reward)\n",
        "\n",
        "def target_height_bonus_reward(physics: mujoco.Physics, object_body_id: int, config: dict) -> float:\n",
        "    \"\"\"Gives a bonus if the object reaches a target height.\"\"\"\n",
        "    object_height = physics.data.xpos[object_body_id][2]\n",
        "    if object_height >= config.get(\"target_height\", 0.06):\n",
        "        return float(config.get(\"target_height_bonus\", 500))\n",
        "    return 0.0\n",
        "\n",
        "def drop_penalty_reward(physics: mujoco.Physics, object_body_id: int, config: dict) -> float:\n",
        "    \"\"\"Penalizes the agent if the object falls too low.\"\"\"\n",
        "    object_height = physics.data.xpos[object_body_id][2]\n",
        "    if object_height < config.get(\"drop_threshold\", 0.02): # Below a very low threshold\n",
        "        return float(-config.get(\"drop_penalty\", 200))\n",
        "    return 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FwPfCE3xsk0"
      },
      "source": [
        "## Reward Composer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "AXjceR9Nx0aP"
      },
      "outputs": [],
      "source": [
        "# src/env/reward_composer.py\n",
        "from typing import Callable, List\n",
        "from dm_control import mujoco\n",
        "\n",
        "class RewardComposer:\n",
        "    \"\"\"\n",
        "    Composes multiple individual reward components into a single total reward.\n",
        "    \"\"\"\n",
        "    def __init__(self, physics: mujoco.Physics, object_body_id: int, reward_config: dict):\n",
        "        self._physics = physics\n",
        "        self._object_body_id = object_body_id\n",
        "        self._reward_config = reward_config\n",
        "\n",
        "        # Define the list of active reward functions to use\n",
        "        self._reward_functions: List[Callable[[mujoco.Physics, int, dict], float]] = [\n",
        "            object_height_reward,\n",
        "            target_height_bonus_reward,\n",
        "            drop_penalty_reward,\n",
        "        ]\n",
        "\n",
        "    def get_total_reward(self) -> float:\n",
        "        \"\"\"Calculates the sum of all active reward components.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        for func in self._reward_functions:\n",
        "            total_reward += func(self._physics, self._object_body_id, self._reward_config)\n",
        "        return total_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u13LoF_G0pEZ"
      },
      "source": [
        "# Instantiation Reward Composer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRyw1QrS0uP5",
        "outputId": "a469d2ff-7ea2-4997-f80f-1118cdc5f467"
      },
      "outputs": [],
      "source": [
        "reward_composer = RewardComposer(physics, manipulable_object.body_id, object_config[\"rewards\"])\n",
        "reward_composer.get_total_reward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfuy2f7x2OkR"
      },
      "source": [
        "## Hand manipulation environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcgaZaaT2V-9",
        "outputId": "0851c8e1-dad2-4ff5-b5bb-2400aaa7fb04"
      },
      "outputs": [],
      "source": [
        "robot.action_space_limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "enzA4DyA2SS5"
      },
      "outputs": [],
      "source": [
        "# src/env/hand_manipulation_env.py\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from dm_control import mujoco\n",
        "import numpy as np\n",
        "\n",
        "class HandManipulationEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Gymnasium environment for Dex Hand object manipulation.\n",
        "    Orchestrates the robot, object, observation, and reward components.\n",
        "    \"\"\"\n",
        "    def __init__(self, physics: mujoco.Physics,\n",
        "                 robot: BaseRobot,\n",
        "                 manipulable_object: BaseObject,\n",
        "                 observation_builder: ObservationBuilder,\n",
        "                 reward_composer: RewardComposer,\n",
        "                 env_config: dict):\n",
        "\n",
        "        super().__init__()\n",
        "        self._physics = physics\n",
        "        self._model = physics.model\n",
        "        self._robot = robot\n",
        "        self._object = manipulable_object\n",
        "        self._observation_builder = observation_builder\n",
        "        self._reward_composer = reward_composer\n",
        "        self._env_config = env_config\n",
        "\n",
        "        self.control_timestep = self._env_config[\"control_timestep\"]\n",
        "        self.episode_duration = self._env_config[\"episode_duration\"]\n",
        "\n",
        "        # Define Observation Space based on the ObservationBuilder\n",
        "        dummy_obs = self._observation_builder.build_observation()\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
        "                                            shape=dummy_obs.shape, dtype=np.float32)\n",
        "\n",
        "        # Define Action Space based on the robot's action space limits\n",
        "        low_action, high_action = self._robot.action_space_limits\n",
        "        self.action_space = spaces.Box(low=low_action, high=high_action, dtype=np.float32)\n",
        "\n",
        "        # Store full initial qpos/qvel for MuJoCo's direct reset,\n",
        "        # then let robot/object components handle their specific parts.\n",
        "        self._initial_qpos_all = self._physics.data.qpos.copy()\n",
        "        self._initial_qvel_all = self._physics.data.qvel.copy()\n",
        "\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self._physics.reset() # Reset MuJoCo's internal state to model defaults\n",
        "\n",
        "        # Reset all qpos/qvel to known initial states (important for deterministic restarts)\n",
        "        self._physics.data.qpos[:] = self._initial_qpos_all\n",
        "        self._physics.data.qvel[:] = self._initial_qvel_all\n",
        "\n",
        "        # Delegate reset logic to specific components\n",
        "        self._robot.reset_state()\n",
        "        self._object.reset_state()\n",
        "\n",
        "        # Step physics a few times to stabilize after reset\n",
        "        for _ in range(10): # Simulate 10 steps to allow physics to settle\n",
        "            self._physics.step()\n",
        "\n",
        "        observation = self._observation_builder.build_observation()\n",
        "        info = {}\n",
        "        return observation, info\n",
        "\n",
        "    def step(self, action):\n",
        "        # Clip actions to stay within the action space boundaries\n",
        "        clipped_action = np.clip(action, self.action_space.low, self.action_space.high)\n",
        "\n",
        "        # Apply actions to the robot's actuators\n",
        "        self._robot.set_commands(clipped_action)\n",
        "\n",
        "        # Simulate for 'num_substeps' to cover 'control_timestep' duration\n",
        "        num_substeps = int(self.control_timestep / self._model.opt.timestep)\n",
        "        for _ in range(num_substeps):\n",
        "            self._physics.step()\n",
        "\n",
        "        reward = self._reward_composer.get_total_reward()\n",
        "\n",
        "        # Check for episode termination (time limit)\n",
        "        terminated = False\n",
        "        if self._physics.data.time >= self.episode_duration:\n",
        "            terminated = True\n",
        "\n",
        "        truncated = False # In gymnasium, time limits typically result in 'truncated=True' but for simplicity initially, 'terminated=True' works.\n",
        "\n",
        "        observation = self._observation_builder.build_observation()\n",
        "        info = {}\n",
        "\n",
        "        return observation, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        # Render the MuJoCo scene for visualization\n",
        "        return self._physics.render(width=640, height=480, camera_id=0) # You might need to adjust camera_id\n",
        "\n",
        "    def close(self):\n",
        "        # No explicit close needed for dm_control.Physics, but good practice\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4FqCYlC4tM5"
      },
      "source": [
        "# Instantiation ENV GYM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKYspj774i6E",
        "outputId": "388fa433-4fc8-461e-afa8-dfcc394e3ef8"
      },
      "outputs": [],
      "source": [
        "env = HandManipulationEnv(\n",
        "        physics=physics,\n",
        "        robot=robot,\n",
        "        manipulable_object=manipulable_object,\n",
        "        observation_builder=observation_builder,\n",
        "        reward_composer=reward_composer,\n",
        "        env_config=object_config[\"env\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfcF-v6UCNdQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWh0Ts-rL4wi"
      },
      "source": [
        "## **Stabe Baseline3 class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ebw-T1PDh5s",
        "outputId": "2a52b7af-e1ea-4e4f-ba13-f5cf34507235"
      },
      "outputs": [],
      "source": [
        "!pip install -q stable_baselines3[extra]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hG9UuRSxCYOW"
      },
      "outputs": [],
      "source": [
        "# src/rl/rl_agent_manager.py\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.env_util import make_vec_env # Not used in this minimal test, but useful\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback\n",
        "import os\n",
        "import gymnasium as gym\n",
        "\n",
        "class RLAgentManager:\n",
        "    \"\"\"Manages the lifecycle of an RL agent (training, saving, loading).\"\"\"\n",
        "\n",
        "    def __init__(self, env: gym.Env, rl_config: dict):\n",
        "        self._env = env\n",
        "        self._rl_config = rl_config\n",
        "        self._model = None\n",
        "\n",
        "        os.makedirs(self._rl_config[\"log_dir\"], exist_ok=True)\n",
        "        os.makedirs(os.path.dirname(self._rl_config[\"model_save_path\"]), exist_ok=True)\n",
        "\n",
        "    def train_agent(self):\n",
        "        \"\"\"Initializes and trains the SAC agent.\"\"\"\n",
        "        print(\"Initializing SAC agent...\")\n",
        "\n",
        "        self._model = SAC(\n",
        "            \"MlpPolicy\",\n",
        "            self._env,\n",
        "            verbose=1,\n",
        "            learning_rate=self._rl_config[\"learning_rate\"],\n",
        "            buffer_size=self._rl_config[\"buffer_size\"],\n",
        "            learning_starts=self._rl_config[\"learning_starts\"],\n",
        "            train_freq=tuple(self._rl_config[\"train_freq\"]),\n",
        "            gradient_steps=self._rl_config[\"gradient_steps\"],\n",
        "            ent_coef=self._rl_config[\"ent_coef\"],\n",
        "            tensorboard_log=self._rl_config[\"log_dir\"]\n",
        "        )\n",
        "\n",
        "        # Callbacks\n",
        "        checkpoint_callback = CheckpointCallback(\n",
        "            save_freq=self._rl_config[\"save_freq\"],\n",
        "            save_path=os.path.join(self._rl_config[\"log_dir\"], \"checkpoints\"),\n",
        "            name_prefix='sac_model'\n",
        "        )\n",
        "\n",
        "        print(f\"Starting training for {self._rl_config['total_timesteps']} timesteps...\")\n",
        "        self._model.learn(\n",
        "            total_timesteps=self._rl_config[\"total_timesteps\"],\n",
        "            progress_bar=True,\n",
        "            callback=[checkpoint_callback]\n",
        "        )\n",
        "        print(\"Training finished!\")\n",
        "        self._model.save(self._rl_config[\"model_save_path\"])\n",
        "        print(f\"Model saved to {self._rl_config['model_save_path']}\")\n",
        "\n",
        "    def evaluate_agent(self, num_episodes: int = 1, render: bool = True):\n",
        "        \"\"\"Evaluates the loaded agent.\"\"\"\n",
        "        if self._model is None:\n",
        "            print(\"No model loaded. Please train or load an agent first.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Evaluating agent for {num_episodes} episodes...\")\n",
        "        for i in range(num_episodes):\n",
        "            obs, info = self._env.reset()\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "            frames = []\n",
        "            while not done:\n",
        "                action, _states = self._model.predict(obs, deterministic=True)\n",
        "                obs, reward, terminated, truncated, info = self._env.step(action)\n",
        "                total_reward += reward\n",
        "                if render:\n",
        "                    frames.append(self._env.render())\n",
        "                done = terminated or truncated\n",
        "            print(f\"Episode {i+1}: Total Reward = {total_reward:.2f}\")\n",
        "            # For Colab, you'd typically save frames and then display them as a video.\n",
        "            # Example (if you have a helper function like display_video):\n",
        "            # if render and frames:\n",
        "            #     from your_utils_file import display_video\n",
        "            #     display_video(frames, fps=1/self._env.control_timestep)\n",
        "        print(\"Evaluation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFbu_TkPCv9J"
      },
      "source": [
        "## **instantiation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e03903eb4101447b875fad53fbe0579a",
            "e9917c847c2f4a7ab5f608a9495c36e6"
          ]
        },
        "id": "_IrEyeJ8C0ZD",
        "outputId": "1838d354-1c0b-49cc-b5bb-6479c738fa5c"
      },
      "outputs": [],
      "source": [
        "rl_manager = RLAgentManager(env, object_config[\"rl\"])\n",
        "print(\"RL Agent Manager initialized.\")\n",
        "\n",
        "# --- Action: Train the agent ---\n",
        "rl_manager.train_agent()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNGMmFUvFY3V"
      },
      "source": [
        "Load training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1zifpFYFb47",
        "outputId": "12fcb367-6060-4e3b-c8ef-102c6a77c5cc"
      },
      "outputs": [],
      "source": [
        "model_path = \"./trained_models/hand_policy.zip\"\n",
        "model = SAC.load(model_path, env=env)\n",
        "print(f\"Model loaded from {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7htKjlTFfYk"
      },
      "source": [
        "Video after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "Sjwhe8NuFiGi",
        "outputId": "13fa0435-de4b-4d03-bfbd-f8a10f206e84"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "from stable_baselines3 import SAC\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Helper function to display video in Colab\n",
        "def display_video(frames, framerate=30):\n",
        "    import imageio\n",
        "    from pathlib import Path\n",
        "    import io\n",
        "\n",
        "    video_path = \"/tmp/simulation.mp4\"\n",
        "    imageio.mimsave(video_path, frames, fps=framerate)\n",
        "\n",
        "    mp4 = open(video_path, 'rb').read()\n",
        "    b64 = b64encode(mp4).decode('ascii')\n",
        "    html = f'''\n",
        "      <video width=640 height=480 controls>\n",
        "            <source src=\"data:video/mp4;base64,{b64}\" type=\"video/mp4\">\n",
        "      </video>\n",
        "    '''\n",
        "    return HTML(html)\n",
        "\n",
        "# Setup\n",
        "duration = 4      # seconds\n",
        "framerate = 30    # Hz\n",
        "max_steps = duration * framerate\n",
        "\n",
        "# Load the trained model\n",
        "model_path = \"./trained_models/hand_policy.zip\"\n",
        "model = SAC.load(model_path, env=env)\n",
        "print(f\"✅ Model loaded from: {model_path}\")\n",
        "\n",
        "# Reset environment\n",
        "obs, info = env.reset()\n",
        "frames = []\n",
        "\n",
        "# For rendering with joint visualization\n",
        "scene_option = mujoco.wrapper.core.MjvOption()\n",
        "scene_option.flags[enums.mjtVisFlag.mjVIS_JOINT] = True\n",
        "\n",
        "# Collect frames\n",
        "for _ in range(max_steps):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # Render and store frame\n",
        "    pixels = physics.render(scene_option=scene_option)\n",
        "    frames.append(pixels)\n",
        "\n",
        "    if terminated or truncated:\n",
        "        print(\"Episode finished early.\")\n",
        "        break\n",
        "\n",
        "# Display video\n",
        "display_video(frames, framerate)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e03903eb4101447b875fad53fbe0579a": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_e9917c847c2f4a7ab5f608a9495c36e6",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">50,401/50,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:01:08</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span> , <span style=\"color: #800000; text-decoration-color: #800000\">737 it/s</span> ]\n</pre>\n",
                  "text/plain": "\u001b[35m 100%\u001b[0m \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50,401/50,000 \u001b[0m [ \u001b[33m0:01:08\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m737 it/s\u001b[0m ]\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "e9917c847c2f4a7ab5f608a9495c36e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
